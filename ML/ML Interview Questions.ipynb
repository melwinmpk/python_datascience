{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/melwinmpk/python_datascience/blob/main/ML/img/ml_basic_flow.jpeg?raw=true\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the Dicision tree works ? or\n",
    "### How does the Gini Calcluation works ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "its a metholidigy which help us to decide which must be the first split as per the Gini Gain Value of the respective colum <br>\n",
    "Higher thr Gini gain among the frist colum that will be the first split <br>\n",
    "Melwin remember the table show the Calcluation <br>\n",
    "Recursive partesioning Method \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why we call the Dicision tree calles as Recursive partesioning Method ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How are you going to make the Gini gain in the continuous Values ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use bucketing in this scenation for the example<br>\n",
    "for the salary we shall set the salary a range and split into buckets then we start to calcluate<br>\n",
    "\n",
    "how are we doing this in the code ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### During which scenarious Tranning Accuracy % and the Test Accuracy % Varies a Lot ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There two scenarious \n",
    "<ul>\n",
    "<li><b>Case 1:</b> If all the Outliers are present in the Test Sample</li>\n",
    "<li><b>Case 2:</b> It Could be the reason for the Over fitting of the Model</li>    \n",
    "</ul>\n",
    "<ul>\n",
    "<li><b>Solution for the Case 1:</b> <br>\n",
    "    Data has to be resampled properly its has to be distributed and it should represent the entire population<br>\n",
    "    <b>Cross Validation:</b>Method to see how well the model performs to unseen data<br></li>\n",
    "<li><b>Solution for the case 2:</b>(Fitst two methods are for avoiding the OverFitting)<br>\n",
    "    <b>Stopping Rule:</b> don’t expand a node if the impurity reduction of the best split is below some threshold<br> \n",
    "    <b>Pruning:</b> grow a very large tree and merge back nodes<br>\n",
    "    reduces the size of decision trees by removing sections of the tree that are non-critical and redundant to classify instances.<br>\n",
    "    \n",
    "</li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Overfitting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is a modeling error that occurs when a function is too closely fit to a limited set of data points.<br>\n",
    "as a result It fails to fit additional data or predict future observations reliably"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which is preffered Accuracy or Area Under the Curve ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It depends on Industry  to Industry <br>\n",
    "For example in the medical field they preffer Area Under the Curve <br>\n",
    "Predecting the Desease as desease is very Important even then there must minimum levelof the accuracy<br>\n",
    "In Case of the Retail shop they would have preffered over the Accuracy <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the one Hot Encoding ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "melwin remember creating the matrix for the encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### why the Randome Forest uses the sample with replacement  ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If its sample with replacement\n",
    "<ul>\n",
    "    <li>There are possibility of infinite no of sample</li>\n",
    "    <li>There will be equal probability of each observations appearing in to data set</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the Random Forest a Black box technique ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes the Random forest is a Black Box technique <br>\n",
    "you will not be knowing which rows and colums are selected in which Sample <br>\n",
    "It Gives a final out put you will not be able to explain to the Buisness team how the how did the final out has arrived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which is better Decision Tree or Randome Forest ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It Depends on Buisness to buisness \n",
    "<ul>\n",
    "    <ul>\n",
    "        <li> Random Forest are more Robest <br>\n",
    "            for the Random forest is a Black box technique as a result It will be dificultto explain what output you are getting\n",
    "        </li>\n",
    "        <li>\n",
    "            with the decision tree diagram its easier to explain and make the buisness understand\n",
    "        </li>\n",
    "    </ul>\n",
    "    <ul>\n",
    "        <li> If the Buisness needs understanding of the decision making then Go for the Decision Tree </li>\n",
    "        <li> if the buisness needs good accuracy , AUC and robestness then go for the Random forest  </li>\n",
    "    </ul>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is Adjsted R square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjested R square mentions the change happened in the R square while adding or removing the variable\n",
    "whith the help of the value dicrease and increase we can identify the variable importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what is the difference between the R square and Adjsted R ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjested R square mentions the change happened in the R square while adding or removing the variable\n",
    "whith the help of the value dicrease and increase we can identify the variable importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Identify what must be the No of Cluster ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by elbo curve or wss plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Handel the Categorical data for the Clustering ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by encoding the values itself we can handel it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference between KNN and K means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th>K-Means</th>\n",
    "        <th>KNN</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>k-Means is Clustering Algorithem</td>\n",
    "        <td>KNN is Clasification Algorithem</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>k-Means is UnSupervised Learning</td>\n",
    "        <td>KNN is Supervised Learning</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>k-means we cannot find the accuracy</td>\n",
    "        <td>KNN we can find the accuracy</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "k-means optmal k value must be found by elbo curve or wss plot</td>\n",
    "        <td>KNN optmal value would be found out by the least error</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "K-Means\t                                                        KNN\n",
    "k-Means is Clustering Algorithem\t                            KNN is Clasification Algorithem\n",
    "k-Means is UnSupervised Learning\t                            KNN is Supervised Learning\n",
    "k-means we cannot find the accuracy\t                            KNN we can find the accuracy\n",
    "k-means optmal k value must be found by elbo curve or wss plot\tKNN optmal value would be found out by the least error\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the simalarities between KNN and K means ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Simalirity ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simalirity = 1/d     (d i.e distance between two data points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### why is the KNN algorithem are known as lazy learners ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Identify the data is linear or not ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the data or the Graph we will not be able to identify weather the data is linear or not<br>\n",
    "we can identify it by  finding the accuracy in the  normal modal and compare it in the SVM <br>\n",
    "if the Accuracy of the SVM if we are able get more accuracy then the data is Non linear "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start using some classification techniques. The thumb-rule is use the simple methods first <br>\n",
    "for e.g. minimum distance classifier, decision trees, naive bayes, and may be SVM with linear kernel (if you understand SVM) <br>\n",
    " If your results are not good, then may be your problem cannot be solved by linear classification methods and you may have to move to more complex non-linear classifiers, such as SVM with Gaussian Kernels, Random Forest, Multi-layer perceptron and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how will we decide when to use SVM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what are the Kernel Trick that are used in SVM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernal trick is the methodology that used in SVM which converts the data from the current dimension to the next dimension till the data is separable, there are scenario wherein the current dimension the data is not resolvable/separable.in case of the no linear data in svm we cannot seperate them in the current diamension so kernal tricks are used to seperate the data.there are different kinds of kernal linear for linear data, Polynomial and Radial for non Linear data. to find weather the data is Linear or not is not easy to find the optimal kernal for the data is a hit and try method along with other hyper parameter grid search CV is avaliable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the non linear data we have to increase the diamension for the the clasifying the Data this type of <br>\n",
    "The goal is that after the transformation to the higher dimensional space, the classes are now linearly separable in this higher dimensional feature space. We can then fit a decision boundary to separate the classes and make predictions. The decision boundary will be a hyperplane in this higher dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@zxr.nju/what-is-the-kernel-trick-why-is-it-important-98a98db0961d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what are C and Gama in SVM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C and Gama is the Regularization parameter <br>\n",
    "C is basically controll the width of the boundery <br>\n",
    "higher the value of the C lower the width of the boundery <br>\n",
    "gamma controls over fitting into the model more the value of the gamma more chances of being overfitting <br>\n",
    "the influence of a single training <br>\n",
    "we are supposed to tune this parameter by hit and try to get the optimal value we can use here the grid search CV as well<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Decision Tree ? (For the Interview Point of View)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Decision Tree is a Supervised learning technique. <br></li>\n",
    "    <li>that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems.<br></li>\n",
    "    <li>It is a tree-structured classifier internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome. <br></li>\n",
    "    <li>Split of the node is done by various technique (Gini gain (CART), Entropy(C4.5) )\n",
    "<pre>\n",
    "Ginigain = Gini (Overall) – Gini (Selected_colum)\n",
    "</pre>\n",
    "So the higest the Ginigain among the variables/columns that will be the Initial split happens (i.e decision rules) <br>\n",
    "The same method continues till the termination condition those are max_depth, min_samples_leaf, min_samples_split \n",
    "    <ul>\n",
    "        <li><b>min_samples_split</b>: minimum number of observations that must exist in a node in order for a split to be attempted.</li>\n",
    "        <li><b>max_depth</b>: The maximum depth of the tree</li>\n",
    "        <li><b>min_samples_leaf</b>: the minimum number of observations in any terminal leaf node. </li>\n",
    "    </ul>    \n",
    "    </li>\n",
    "    <li>To tune the hyperparameter  for max_depth, min_samples_leaf, min_samples_split we can do hit and try we can even Use <b>GridSearchCV</b></li>\n",
    "    <li>Decision Tree is one of the Easily Explanable Algorithem to the Buisness team</li>\n",
    "</ul>    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Random forest ? (Interview point of view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Random forest is a Supervised learning technique.</li>\n",
    "    <li>That can be used for both classification and Regression problems</li>\n",
    "    <li>It is an Extension of decision tree. Decision tree will not be accurate if there is changes in sample over period of time. Random forrest is robust and is not affected by the changing ratio in target variable</li>\n",
    "    <li>Random Forest Classifier is an <b>ensemble algorithm</b>,(i.e use of multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms)<br>\n",
    "        <ul>\n",
    "            <li>which creates a set of decision trees from a randomly selected subset of the training set, <br>\n",
    "                (randomly selecting sample method is known as <b>Bootstrap aggregating</b>(Random Sample with Replacement), also called <b>bagging</b>) <br></li>\n",
    "                <li>which then aggregates the votes from different decision trees to decide the final class of the testobject</li></ul>\n",
    "    </li>\n",
    "    <li>To decide the optimal no of the decision trees (i.e n_estimators) we have <b>Out-of-bag (OOB) error</b>  is a method of measuring the prediction error of random forests</li>\n",
    "    <li>There are parameters that have to be tuned to get the optimal value like max_depth, crierion, max_features we can use <b>GridSearchCV</b></li>\n",
    "    <li>Random Forest is Black technique box but useful in retail kind of applications where overall accuracy is more important</li>\n",
    "    <li>As its a Black Box technique its not easily explainable to the business team</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is Linear Regression ? (Interview Point of view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Linear Regression is a supervised machine learning algorithm</li>\n",
    "    <li>Linear Regression is used to solve only Regression Problems</li>\n",
    "    <li>linear regression is a method of finding the best straight line fitting to the given data, i.e. finding the best linear relationship between the independent and dependent variables.</li>\n",
    "    <li>In technical terms, For the Optimal Line it uses <b>Residual Sum of Squares </b><b>[square of (y^ - y)]</b> methos(A residual sum of squares (RSS) measures the level of variance in the error term, or residuals, of a regression model) Ideally, the RSS should be a smaller or lower value than the sum of squares from the regression model's inputs. </li>\n",
    "    <li>Linear Regression is highly effected by out liers make sure that the data is cleaned</li>\n",
    "    <li>For the Linear Regression Scaling has to be done to the Data before applying the model</li>\n",
    "    <li>We have <b>Ridge</b> and <b>Lasso</b> which are different version of Linear regressiom</li>\n",
    "    <li>In the Normal Linear Regression there are high chances that the best filt line that got trained from Test data lead to Overfitting which can fail in the Test Data</li>\n",
    "    <li>In Ridge we <b>add [lamda * Square(slope)] along with Residual Sum of square to find the minimum</b> Its generally used to make the slope less steaper this method will help to less the over fitting towards the Training data set (landa value will be > 0 any positive)</li>\n",
    "    <li>In Lasso its same as Ridge we <b>add [landa * absolute(slope)] along witht the Residual Sum of square to find the minimum</b> . It helps in reducing the overfitting as well as it helps in feature selection</li>\n",
    "</ul>\n",
    "<a href=\"https://medium.com/analytics-vidhya/preparing-for-interview-on-machine-learning-3145caeea06b\">Medium link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### What are the assumptions made in linear regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>There should be a linear and additive relationship between dependent (response) variable and independent (predictor) variable(s). A linear relationship suggests that a change in response Y due to one unit change in X is constant, regardless of the value of X. An additive relationship suggests that the effect of X on Y is independent of other variables.\n",
    "    <br>\n",
    "     <b>Linear and Additive:</b> If we fit a linear model to a non-linear and non-additive data set, the regression algorithm would fail to capture the trend mathematically, thus resulting in an inefficient model. Also, this will result in erroneous predictions on an unseen data set.       \n",
    "    </li>\n",
    "    <li>There should be no correlation between the residual (error) terms.\n",
    "        <br>\n",
    "    <b>Autocorrelation:</b> Autocorrelation occurs when the residuals are not independent from each other. In other words when the value of y(x+1) is not independent from the value of y(x).The presence of correlation in error terms drastically reduces model’s accuracy.\n",
    "    </li>\n",
    "    <li>The independent variables should not be correlated.\n",
    "    <br>\n",
    "    <b>Multicollinearity:</b> This phenomenon exists when the independent variables are found to be moderately or highly correlated. In a model with correlated variables, it becomes a tough task to figure out the true relationship of predictors with response variable. In other words, it becomes difficult to find out which variable is actually contributing to predict the response variable.    \n",
    "    </li>\n",
    "    <li>The error terms must have constant variance. This phenomenon is known as homoskedasticity.</li>\n",
    "    <li>The error terms must be normally distributed.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is the KNN Algorithem ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>K-Nearest Neighbors is the supervised machine learning algorithm used for classification and regression</li>\n",
    "<li>KNN will label(classify) new data points based on the 'k' number of nearest data points </li>\n",
    "<ul>\n",
    "    <li>Initialize the K value.</li>\n",
    "    <li>Calculate the distance between test input and K trained nearest neighbors. <b>(Euclidian Distance, Manhattan distance)</b></li>\n",
    "    <li>Check class categories of nearest neighbors and determine the type in which test input falls.</li>\n",
    "    <li>Classification will be done by taking the majority of votes.</li>\n",
    "    <li>Return the class category.</li>\n",
    "</ul>    \n",
    "<li>to find the optimal K we use Derive a plot between error rate and K denoting values in a defined range. Then choose the K value as having a minimum error rate.  </li>\n",
    "<li>KNN is a lazy learning algorithm (We have to compute distances between test points and trained labels points. Updating distance metrics with every iteration is computationally expensive as a result its known as Lazy learning algorithem)</li>\n",
    "<li>Scaling the Data is Mandatory</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is SVM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Support Vector Machine is a supervised machine learning algorithm</li>\n",
    "    <li>SVM can be used for Both classification or regression challenges. However, it is mostly used in classification problems</li>\n",
    "    <li>SVM can be used for both Linearly Seperabely and Non Linearly Seperablely Data</li>\n",
    "    <li>For Linearly Seperabely Data <br>\n",
    "the SVM creates the Hyper Plane that has higest <b>Marginal Distance</b> to seperate the data</li>\n",
    "    <li>For Non Linearly Seperable Data <br>\n",
    "        SVM uses the <b>Kernal Method</b> to Increase the Diamension till the data is Linearly seperabel then its sets the Hyper plane with the higest <b> Marginal Distance </b> to Seperate the Data\n",
    "    </li>\n",
    "        <li>there are differnet Kernal methods <br> <b>Polynomial Kernel</b> – Non-Linear Data Classification <br> <b>Radial or RBF Kernel</b> – Non-Linear Data Classification <br>  <b>Linear Kernel</b> – Linear Data Classification</li>\n",
    "    <li>Hyper Parameter C (Regularization) & Gamma <br>\n",
    "        C represents the width of the\n",
    "margin for classification purpose <br>\n",
    "       gamma parameter defines how far\n",
    "the influence of a single training <br>\n",
    "        Optimal Value of gamma needs to be included\n",
    "otherwise Model can run into over fitting\n",
    "    </li>\n",
    "    <li>for the Optimal Value of the Hyper Parameter selection we use Gridsearch CV  </li>    \n",
    "    <li>SVM is Computationally Expensive</li>\n",
    "    <li>Its a Black Box Technique explanibility will be difficult to the Buisness team</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Naive Bayesian Classifier ? (Interview point of View)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Naive Bayesian Classifier(NBC) is a supervised machine learning algorithm</li>\n",
    "    <li>NBC can be used for Both classification or regression challenges. However, it is mostly used in classification problems</li>\n",
    "    <li>It is a classification technique based on Bayes' Theorem with an <b>assumption of independence among predictors.</b><br>Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.</li>\n",
    "    <li>NBC does not effect with the Out liers or Coorilation</li>\n",
    "    <li>NBC Uses the Formuls.<br>\n",
    "        <img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2015/09/Bayes_rule-300x172-300x172.png\"></img>\n",
    "        <ul>\n",
    "            <li>P(c|x) is the posterior probability of class (c, target) given predictor (x, attributes).</li>\n",
    "            <li>P(c) is the prior probability of class.</li>\n",
    "            <li>P(x|c) is the likelihood which is the probability of predictor given class.</li>\n",
    "            <li>P(x) is the prior probability of predictor.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>In NBC we caluclate using this formula for eact attribute and multiply we caluclate when the target is 0 and the tagrt is 1 higest probability among these we assign the clasification</li>\n",
    "    <li> Naive Bayes classifiers mostly used in text classification (due to better result in multi class problems and independence rule) have higher success rate as compared to other algorithms. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Logestic Regression in Ontervierw point of view ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Logistic Regression is a Supervised Machine Learning Algorithem</li>\n",
    "    <li>Logistic Regression can be used for Classification Problem </li>\n",
    "    <li>Logistic regression becomes a classification technique only when a decision threshold is brought into the picture.</li>\n",
    "    <li>weight caluclation is done by Odds Ration which is done  y = w1x1 + w2x2 + w3x3 ...</li>\n",
    "    <li>odds =  probability/(1-probability)  =  p/q </li>\n",
    "    <li>Logistic Regression uses Sigmoid Function (known as activation function) some liberary uses Softmax Function </li>\n",
    "    <li><span>Sigmoid Function:</span><img src=\"https://www.gstatic.com/education/formulas2/355397047/en/sigmoid_function.svg\"></img></li>\n",
    "    <li>From Sigmoid function we decide the threshold based on the data (for some example Data is used for) to decide 0 or 1 (in case of Binary Clasificartion )</li>\n",
    "    <li>Standardization isn't required for logistic regression</li>\n",
    "    <li>works best on binary classification problems, although it can be used on multi-class classification problems through the “one vs. all” method</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is K Means Algorithem ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>K - Means is a Unsupervised Clustering Learning Algorithem</li>\n",
    "<li>K - Means Algorithem tries to partition a set of points into K sets (clusters) such that the points in each cluster tend to be near each other.</li>\n",
    "<li> Algorithem Steps:</li>\n",
    "    <ul>\n",
    "        <li>1.Assume K Centroids (for K Clusters)</li> \n",
    "        <li>2.Compute Eucledian distance of each objects with these Centroids.</li>\n",
    "        <li>3.Assign the objects to clusters with shortest distance </li>\n",
    "        <li>4.Compute the new centroid (mean) of each cluster based on the objects assigned to each clusters.<br> The K number of means obtained will become the new centroids for each cluster\n",
    "        <li>5.Repeat step 2 to 4 till there is convergence</li>\n",
    "        <li>-i.e. there is no movement of objects from one cluster to another</li> \n",
    "        <li>-Or threshold number of iterations have occurred</li>\n",
    "    </ul>  \n",
    "<li>To Use the Optimal No of Cluster we can use <b>Elbo Curve or WSS Plot</b> ( For each value of k we then compute the <b>sum of squared errors (SSE)</b> and add both into a line plot. We want to choose k so that we have a small SSE, but as we increase k the SSE tends to decrease towards 0  ) </li>\n",
    "<li>K-Means clustering method considers two assumptions regarding the clusters</li>\n",
    "<ul>– first that the clusters are spherical and second that the clusters are of similar size.<br> Spherical assumption helps in separating the clusters when the algorithm works on the data and forms clusters</ul>    \n",
    "<li>Smilearly we have <b>K -Means ++</b> which overcomes the Errors caused by the random selection of the initial centroid.<br> \n",
    "    In K-means++ you pick the initial centroids using an algorithm that tries to initialize centroids that are far apart from each other.\n",
    "</li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
