{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is Resudual Sum of Square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.gstatic.com/education/formulas2/355397047/en/residual_sum_of_squares.svg\"></img>\n",
    "\n",
    "RSS\t=\tresidual sum of squares <br>\n",
    "y_i\t=\ti^th value of the variable to be predicted <br>\n",
    "f(x_i)\t=\tpredicted value of y_i <br>\n",
    "n\t=\tupper limit of summation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A residual sum of squares (RSS) measures the level of variance in the error term, or residuals, of a regression model. Ideally, the sum of squared residuals should be a smaller or lower value than the sum of squares from the regression model's inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is Total Sum of Square ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total variation in target variable is the sum of squares of the difference between the actual values and their mean.<br>\n",
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/07/TSSchange.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### how is the R Squared is Caluclated ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "R-squared = (TSS-RSS)/TSS\n",
    "\n",
    "          = Explained variation/ Total variation\n",
    "\n",
    "          = 1 â€“ Unexplained variation/ Total variation\n",
    "</pre>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared gives the degree of variability in the target variable that is explained by the model or the independent variables. If this value is 0.7, then it means that the independent variables explain 70% of the variation in the target variable <br>\n",
    "R-squared value always lies between 0 and 1. A higher R-squared value indicates a higher amount of variability being explained by our model and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is adjested r square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is the difference between r square and adjested r square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is Cost Function , Gradient Decent, Global Minimum ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Cost function is Resedual sumof square in the Linear regression \n",
    "from cost function we develope the Gradient decent graph with different slope selected</p>\n",
    "<p>in Gradient decent we find the Global Minimum using Convergance theorem  which has the learning rate with help of that we reach towards the \n",
    "Global Minium</p>\n",
    "<p>from the gloabla minium we ge the optimal Slop value in which the co</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=1-OGRohmH2s&list=PLZoTAELRMXVPBTrWtJkn3wWQxZkmTXGwe&index=34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is Linear Regression ? (Interview Point of view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Linear Regression is a supervised machine learning algorithm</li>\n",
    "    <li>Linear Regression is used to solve only Regression Problems</li>\n",
    "    <li>linear regression is a method of finding the best straight line fitting to the given data, i.e. finding the best linear relationship between the independent and dependent variables.</li>\n",
    "    <li>In technical terms, For the Optimal Line it uses <b>Residual Sum of Squares </b> <b>[square of (y^ - y)]</b> methos(A residual sum of squares (RSS) measures the level of variance in the error term, or residuals, of a regression model) Ideally, the RSS should be a smaller or lower value than the sum of squares from the regression model's inputs. </li>\n",
    "    <li>Linear Regression is highly effected by out liers make sure that the data is cleaned</li>\n",
    "    <li>For the Linear Regression Scaling has to be done to the Data before applying the model</li>\n",
    "    <li>We have <b>Ridge</b> and <b>Lasso</b> which are different version of Linear regressiom</li>\n",
    "    <li>In Ridge we add <b>[lamda * (Square of slope)] along with Residual Sum of square to find the minimum</b> Its generally used to make the slope less steaper this method will help to less the over fitting towards the Training data set (landa value will be > 0 any positive)</li>\n",
    "    <li>In Lasso its same as Ridge we add <b>[landa * absolute(slope)] along witht the Residual Sum of square to find the minimum</b> . It helps in reducing the overfitting as well as it helps in feature selection</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Very Important Link</b></p>\n",
    "Preparing for interview on Machine Learning? Here, is a complete guide to interview questions on Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/analytics-vidhya/preparing-for-interview-on-machine-learning-3145caeea06b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
